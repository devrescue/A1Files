# Postgres Integration with Jupyter Notebook

Postgres or PostgreSQL is a free and open source object-relational database management system. It's very similar to traditional RDBMS systems such as SQL Server and MySQL except that it includes Object Oriented concepts and modelling in its architecture and functionality. Postgres was ranked the number one most popular database among professional developers in the [2022 Stack Overflow Developer Survey](https://survey.stackoverflow.co/2022/#most-popular-technologies-database-prof "Go To Survey") .

Jupyter Notebook is a web application that combines live code, data and visualizations into interactive, dynamic documents called _notebooks_.

What you will learn in this article is how to use Jupyter Notebook to query, manipulate and seamlessly integrate data and functionality from a PostgreSQL database into a Jupyter Notebook. Your notebook will contain the live code that will extract and maipulate rows from the database for simple Exploratory Data Analysis (EDA).

This is the plan of action you will follow:

- Set Up Environment and Load Sample Data
- Create Python Notebook and Integrate Postgres

---

## Set Up Environment and Load Sample Data

The first step would be to prepare the environment. You will need to have the following up and running on your workstation before moving forward:

- PostgreSQL 14.3 (or recent version)
- Jupyter Notebook (recent version)

Setting up the environment may take a fair amount of work. For this article, both Jupyter Notebook and Postgres were set up on the same Ubuntu 22.04 LTS (Jammy Jellyfish) VM for simplicity.

You have the option of setting up a completely new Ubuntu 22.04 VM environment if any of the following steps prove problematic on your usual OS.

### PostgreSQL Set Up

You can download and install PostgreSQL for your OS at the [official site](https://www.postgresql.org/download/linux/ubuntu/ "Go To Page"). Ensure that your PostgreSQL instance is configured to allow incoming connections. Consult the [documentation and instructions](https://www.postgresql.org/docs/14/index.html "Go To Page") for your version if you get stuck setting it up.

For this article, the [World Health Statistics 2020 dataset](https://www.kaggle.com/datasets/utkarshxy/who-worldhealth-statistics-2020-complete "Go To Page") will be used. It was originally posted on Kaggle but a small subset of this data is available at [this Github repo](https://github.com/devrescue/postgres-sampledb "Go To Repo") for easy import into Postgres. Alternatively, you can adapt the upcoming example code to use whatever data you want.

If you are using the sample data, clone the repo to your workstation and edit the CSV locations in the _import.sql_ file to point to the absolute paths. On Ubuntu (22.04) importing the data would then be as simple as running the following commands at the Terminal:

```shell
sudo -i -u postgres psql -c "CREATE DATABASE \"sampledb\";"
sudo -i -u postgres psql -d sampledb < create_tables.sql
sudo -i -u postgres psql -d sampledb < import.sql
```

The first command creates the _sampledb_ database. The second command creates the schema and tables and the third copies the rows from the CSV files into the database tables. Ensure that you have a user with full permissions on this database.

### Jupyter Notebook Set Up

Next, ensure that Jupyter Notebook is installed and functional. Reference the [installation instructions](https://jupyter.org/install "Go To Page") for more details but it's usually as simple as running the following command at the terminal:

```shell
pip install notebook
```

And you launch it with:

```shell
jupyter notebook
```

Installing Jupyter Notebook will alo install the _IPython Kernel_. A _kernel_ is what Jupyter Notebook uses to provide programming language support. The _IPython Kernel_ provides support for Python programming language notebooks via _IPython_, the enhanced interactive Python shell.

When you launch Jupyter Notebook you should see the following options under _New_. The notebook you will soon create will be a Python 3 notebook:

![New Notebook!](https://i.imgur.com/frWdcRm.png "New Notebook")

<!-- un-annotated version @ [Imgur](https://i.imgur.com/nK9IjqT.png)
-->

Quit Jupyter Notebook and install the [IPython SQL extension](https://github.com/catherinedevlin/ipython-sql "Go To Repo") which will allow you to use SQL commands from within Jupyter Notebook. The command to install is:

```shell
pip install ipython-sql
```

Now install the [pgspecial package](https://github.com/dbcli/pgspecial "Go To Repo") which allows us to execute meta-commands against the PostgreSQL instance such as _\l_ (list all DBs), _\dt_ (list all tables), _\dn_ (list all schemas) etc.

```shell
pip install pgspecial
```

Install the [psycopg PostgreSQL database adapter](https://www.psycopg.org/docs/install.html#quick-install "Go To Page") for Python with the following command. This is very important:

```shell
pip install psycopg2-binary
```

Finally, you'll install a few additional Python libraries that will be used for the upcoming example code:

```shell
pip install pandas
pip install matplotlib
pip install seaborn
```

You should recognize these three well knowm Python libraries: _pandas_ is used for its versatile and powerful _DataFrames_, _matplotlib_ and _seaborn_ are used to create impactful charts and other visualizations.

---

## Create Python Notebook and Integrate PostgreSQL Data

With the environment now prepared, launch Jupyter Notebook and create a New Python 3 Notebook. Navigate to the _New_ button on the right, click it then select _Python 3 (ipykernel)_ or similar.

The _ipykernel_ is a _kernel_ of our Python Notebook. You can think of it as the "brain" of the notebook document.

You will now use the IPython SQL extension installed earlier to connect to the Postgres database and perform some simple Postgres specific meta-commands as well as some DML and DDL SQL operations:

First, load the extension with the following magic command in a new cell in the notebook:

```python
%load_ext sql
```

Magic commands or _magics_ are built in enhancements to the kernel that add special functionality to the notebook. For Python notebooks, they are preceded by a _%_ if on one line (line magic) of code or _%%_ if on multiple lines of code (cell magics).

Next, you will connect to your PostgreSQL database with the following magic command. The _%sql_ syntax indicates that this is a line magic.:

```python
%sql postgresql://postgres:*******@localhost/sampledb
```

That was a [SQLAlchemy URL connection string](https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls "Go To Page") magic command. The URL connection string is presented in the following format:

```python
dialect://username:password@host/database
```

Where:

- _dialect_ is the DB server type which is _postgres_
- _username_ is the DB account username which is also _postgres_ (may be different for you)
- _password_ is the DB account password (change to real password)
- _host_ is the server name which is _localhost_ (may be different for you)
- _database_ is the database name _sampledb_

The username you use in the connection string _must_ have the permissions to SELECT, INSERT, UPDATE and DELETE rows in the _sampledb_.

Now try some meta-commands. Meta-commands allow you to perform administrative type functions against the Postgres instance easily and quickly. Below only 3 are shown (_\l_, _\dn_, _\dt_) but there are [a lot more](https://www.postgresql.org/docs/current/app-psql.html#:~:text=removed%20by%20psql.-,Meta%2DCommands,called%20slash%20or%20backslash%20commands. "Go To Page") :

![PostgreSQL Meta-Commands](https://i.imgur.com/hz6ghwP.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/zCDCmIX.png)
-->

Next, you will run a simple SELECT command against the _sampledb_:

```sql
SELECT * FROM HealthStats.TobaccoAge LIMIT 10
```

If your connection to the database was successful and the sample data was imported correctly you should see 10 rows returned from the _HealthStats.TobaccoAge_ table:

![IPython SQL Extension Query Results](https://i.imgur.com/veWURO7.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/KVFs07y.png)
-->

Query results are loaded as a list. It's good practice to limit your rows during testing so you don't use too much memory.

INSERT, UPDATE and DELETE statements also work as expected:

```sql
INSERT INTO HealthStats.TobaccoAge ("Location", "Indicator", "Period", "Dim1", "First Tooltip")
VALUES ('TEST','TEST',9999,'TEST',0.00)
RETURNING *

UPDATE HealthStats.TobaccoAge SET "First Tooltip" = 9999.99
WHERE "tobaccoage"."Location" = 'TEST'
RETURNING *

DELETE FROM HealthStats.TobaccoAge WHERE "tobaccoage"."Location" = 'TEST'
```

![DML STATEMENTS](https://i.imgur.com/2JMJRZr.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/zc1NjjX.png)
-->

Recall that the _%%sql_ syntax indicates a cell magic and is required for multiple lines of code.

JOINS also work as expected:

```sql
SELECT
a."Location" as "HALE Location",
a."Dim1" as "Sex",
a."First Tooltip" as "HALE at Birth (Years)",
b."First Tooltip" as "Medical Doctors (per 10,000)",
a."Period"
FROM HealthStats.HealthyLifeExpectancyAtBirth a
INNER JOIN (SELECT * FROM HealthStats.MedicalDoctors WHERE "Period" = 2010) b
ON a."Location" = b."Location"
WHERE a."Period" = 2010 and a."Dim1" = 'Both sexes'
ORDER BY a."Location"
LIMIT 10
```

This is a simple INNER JOIN on two tables from the _sampledb_. When you execute that query you get the following result:

![INNER JOIN QUERY](https://i.imgur.com/YUuT57q.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/tVcjg6m.png)
-->

This SQL extension also works well with Pandas DataFrames. It is possible to return the results of a SQL extension magic command as a pandas Dataframe:

![SQL EXTENSION RESULT TO DATAFRAME](https://i.imgur.com/mgbuZ79.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/gHgrGlB.png)
-->

Once the _result_ is returned, the local variable can be stored as _df_ and treated as a regular dataframe.

DDL operations such as CREATE VIEW are also possible. You will now re-write the INNER JOIN query to include 2 additional tables and save it as a new VIEW on the _HealthStats_ schema in the _sampledb_ database:

![INNER JOIN ON THREE TABLES](https://i.imgur.com/zezGIbn.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/atvoArO.png)
-->

You can now close the PostgreSQL database connection with the following code. The first line uses a magic command to returns all the current DB connections and the second line terminates all the DB connections returned from the first:

```python
connections = %sql -l
[c.session.close() for c in connections.values()]
```

Another approach to integrating Postgres data would be connect to the PostgresSQL DB and use the _psycopg_ library within a Python script. Run the following in a new cell:

```python
import psycopg2

try:
    conn = psycopg2.connect("host=localhost dbname=sampledb user=postgres password=*****")

    cur = conn.cursor()
    cur.execute("SELECT * FROM HealthStats.BasicDrinkingWater LIMIT 20")
    result = cur.fetchall()
    print(result)

except psycopg2.DatabaseError as e:
    print(f'Error: {e}')
    sys.exit(1)
finally:
    if conn:
        conn.close()
```

This approach may be the most popular way to integrate Postgres with Jupyter Notebook. When you execute the above code in a new cell your, output would look something like this:

```text
[('Afghanistan', 2017, 'Population using at least basic drinking-water services (%)', 57.32), ('Afghanistan', 2016, 'Population using at least basic drinking-water services (%)', 54.84)...
```

That may not look like much but Python scripts that integrate Postgres data can accomplish great things! Not only can you use _pandas_ to create dataframes out of the raw data for querying and manipulation, but you can leverage other powerful libraries such as _seaborn_ and _matplotlib_ for quantitative analysis and visualizations.

Re-writing the above Python script to create 3 plots:

```python
import psycopg2

import pandas as pd
import pandas.io.sql as sqlio

import matplotlib.pyplot as plt
import seaborn as sns

try:
    conn = psycopg2.connect("host=localhost dbname=sampledb user=postgres password=*****")
    sql_query = 'SELECT * FROM HealthStats.\"STATS_2010\"'
    df = sqlio.read_sql_query(sql_query,conn)
except psycopg2.DatabaseError as e:
    print(f'Error {e}')
    sys.exit(1)
finally:
    if conn:
        conn.close()

sns.set_theme(color_codes=True)

# HALE vs Doctors per 10,000, HALE = Healthy Age Life Expectancy
sns.lmplot(x="HALE at Birth (Years)",
           y="Medical Doctors (per 10,000)",
           fit_reg = True,
           hue="HALE at Birth (Years)",
           palette = "coolwarm_r",
           legend = False,
           data=df)
plt.show()

# HALE vs Prevalence (%) of Tobacco Smoker over 15 years
sns.lmplot(x ="HALE at Birth (Years)",
           y="Prev. of Tobacco Smoking (15 plus)",
           data=df)
plt.show()


# HALE vs % Population with Basic Drinking Water Service
sns.lmplot(x="HALE at Birth (Years)",
           y="% Pop Basic Drinking Water Serv.",
           data=df)
plt.show()
```

Essentially this code is importing _pandas_, _matplotlib_ and _seaborn_ libraries, connecting to the Postgres database, extracting rows and drawing 3 simple scatterplots from the _STATS_2010_ VIEW that was created earlier.

These plots can be used as part of your Exploratory Data Analysis to investigate likely relationships between the variables in the dataset.

It appears that for the World Health Statistics 2010 dataset there is a clear positive coorelation between Healthy Age Life Expectancy and Medical Doctors Per 10,000 as well as percentage of population with basic drinking water services. But for the Prevalence of Tobacco Smoking in 15 and over people, there is a weaker coorelation.

These elementary insights are possible but also much more. As the data (both and live and static) in your Postgres database increases over time, you can stay abreast of how these relationships evolve and even possibly discover new ones.

![SCATTER PLOT USING POSTGRE](https://i.imgur.com/iBI8BK0.png)

<!-- non-annotated version @ [Imgur](https://i.imgur.com/BMapLas.png)
-->

---

## Wrapping Up

In this article you learned how to integrate Jupyter Notebook with PostgreSQL by:

1. Using the IPython SQL extension magic commands _%_ and _%%_ to connect to a PostgreSQL database to perform DML and DDL operations from a Notebook.
2. Using the _pgspecial_ package to perform meta-commands against a PostgreSQL database from a Notebook.
3. Using a Python Script that leverages the power of _pandas_, _seaborn_ and _matplotlib_ libraries to extract data from PostgreSQL for visualzation and analysis in a Notebook.

This is just skimming the surface. Using Jupyter Notebook you can potentially implement entire Data Science workflows using live or static Postgres data. In Machine Learning, for example, you can perform everything from Data Wrangling and Data Cleaning to Exploratory Data Analysis to Feature Engineering to training and testing ML Models based on live or static Postgres data.

Thanks for reading and good luck in all your Data Science adventures!
